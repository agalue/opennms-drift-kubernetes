# @author Alejandro Galue <agalue@opennms.org>
#
# NOTES:
# - This manifest is designed to work with the unaltered version of the image from:
#   https://hub.docker.com/r/opennms/horizon-core-web
#   https://github.com/opennms-forge/docker-horizon-core-web
# - It is perfectly valid to have all the required configuration files on a config-map,
#   and then mount those files to the overlay directory, instead of using an initContainer.
#   That said, without the intelligence to guard against upgrades through the initContainer
#   script, if you opt for using config-maps, you have to prepare the config files on the PV
#   prior upgrading the version of OpenNMS on the chosen image.
#
# WARNING:
# - This deployment will have a persistent volume for /opt/opennms/etc to track the configuration changes.
# - The opennms process runs as root (for H23 or older), which is not allowed on OpenShift.
# - The default images are based on OpenJDK, but Oracle JDK is recommended for production.
# - The image tag bleeding refers to the upcoming version of OpenNMS (unstable).
# - In order to execute ICMP requests as non-root a recent kernel on the K8s Hosts is required.
#   For H24+, to force OpenNMS to run as root, add the following to the container template:
#   securityContext:
#     runAsUser: 0
#
# Java 8 Issues:
# - -XX:+UnlockExperimentalVMOptions and -XX:+UseCGroupMemoryLimitForHeap won't work
# - -XX:ParallelGCThreads and -XX:ConcGCThreads might require to be updated.
#    Although, Runtime.getRuntime().availableProcessors() seems to respect the CPU limitations.
#
# TODO:
# - Potential tools for managing /opt/opennms/etc:
#   https://vapor-ware.github.io/ksync/
#   https://github.com//kubernetes/git-sync
#   https://gitkube.sh
# - Add a lifecycle.preStart to execute a "git commit" of the current state of the configuration,
#   in order to track what has been changed between since the last time the Pod was running.

---
apiVersion: v1
kind: Service
metadata:
  name: opennms-core
  namespace: opennms
  labels:
    app: onms
    deployment: drift
spec:
  clusterIP: None
  ports:
  - port: 8980
    name: http
  - port: 8101
    name: karaf
  - port: 2049
    name: nfs
  selector:
    app: onms

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: onms
  namespace: opennms
  labels:
    app: onms
    deployment: drift
spec:
  serviceName: opennms-core
  replicas: 1 # The solution only allows 1 instance
  selector:
    matchLabels:
      app: onms
  template:
    metadata:
      labels:
        app: onms
        deployment: drift
    spec:
      terminationGracePeriodSeconds: 120
      initContainers:
      # Requires a fairly recent Kernel on the K8s host
      - name: init-sysctl
        image: busybox
        command: # Allow ICMP as non-root
        - sysctl
        - -w
        - net.ipv4.ping_group_range=0 429496729
        securityContext:
          privileged: true
      # Initialize OpenNMS Common Configuration
      # Requires the same image/version used at runtime: horizon-core-web
      - name: init-config
        image: opennms/horizon-core-web:23.0.3-1
        imagePullPolicy: IfNotPresent
        command: [ bash, /init.sh ]
        securityContext: # Required to fix ownership when OpenNMS is not running as root
          runAsUser: 0
        env:
        - name: TZ
          valueFrom:
            configMapKeyRef:
              key: TIMEZONE
              name: common-settings
        - name: INSTANCE_ID
          valueFrom:
            configMapKeyRef:
              key: OPENNMS_INSTANCE_ID
              name: common-settings
        - name: FEATURES_LIST
          value: opennms-es-rest,opennms-kafka-producer
        - name: CASSANDRA_SERVER
          value: cassandra.opennms.svc.cluster.local
        - name: CASSANDRA_REPFACTOR
          valueFrom:
            configMapKeyRef:
              key: CASSANDRA_REPLICATION_FACTOR
              name: common-settings
        - name: KAFKA_SERVER
          value: kafka.opennms.svc.cluster.local
        - name: ELASTIC_SERVER
          value: esdata.opennms.svc.cluster.local
        - name: ELASTIC_PASSWORD
          valueFrom:
            secretKeyRef:
              key: ELASTICSEARCH_PASSWORD
              name: onms-passwords
        volumeMounts:
        - name: opennms-etc # Persistent volume for OpenNMS Configuration
          mountPath: /opennms-etc
        - name: opennms-config
          mountPath: /init.sh
          subPath: onms-core-init.sh
      # Initializes Newts keyspace on Cassandra/ScyllaDB through CQLSH (requires newts.cql)
      - name: init-newts
        image: cassandra:3.11.4
        imagePullPolicy: IfNotPresent
        command: [ sh, -c, 'cqlsh -f /opennms-etc/newts.cql $CASSANDRA_HOST' ]
        env:
        - name: CASSANDRA_HOST
          value: cassandra.opennms.svc.cluster.local
        volumeMounts:
        - name: opennms-etc
          mountPath: /opennms-etc
      - name: dependencies
        image: waisbrot/wait
        imagePullPolicy: IfNotPresent
        env:
        - name: TARGETS
          value: postgresql.opennms.svc.cluster.local:5432,cassandra.opennms.svc.cluster.local:9042,kafka.opennms.svc.cluster.local:9092,esdata.opennms.svc.cluster.local:9200
        - name: TIMEOUT
          value: '600'
      containers:
      - name: onms
        image: opennms/horizon-core-web:23.0.3-1
        imagePullPolicy: IfNotPresent
        args:
        - -s
        ports:
        - containerPort: 8101
          name: karaf
        - containerPort: 8980
          name: http
        env:
        - name: TZ
          valueFrom:
            configMapKeyRef:
              key: TIMEZONE
              name: common-settings
        - name: MEM_TOTAL_MB
          valueFrom:
            resourceFieldRef:
              resource: requests.memory
              divisor: 1Mi
        - name: JAVA_OPTS # Preferred way instead of opennms.conf
          value: -XX:+UseG1GC -Xms$(MEM_TOTAL_MB)m -Xmx$(MEM_TOTAL_MB)m
        - name: POSTGRES_HOST
          value: postgresql.opennms.svc.cluster.local
        - name: POSTGRES_PORT
          value: '5432'
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              key: POSTGRES_PASSWORD
              name: onms-passwords
        - name: OPENNMS_DBNAME
          value: opennms
        - name: OPENNMS_DBUSER
          value: opennms
        - name: OPENNMS_DBPASS
          valueFrom:
            secretKeyRef:
              key: OPENNMS_DB_PASSWORD
              name: onms-passwords
        volumeMounts:
        - name: opennms-etc
          mountPath: /opt/opennms/etc
        resources:
          limits:
            cpu: 4
            memory: 6Gi
          requests:
            cpu: 4
            memory: 4Gi
        readinessProbe:
          httpGet:
            path: /opennms/login.jsp
            port: http
          initialDelaySeconds: 15
          periodSeconds: 15
        livenessProbe:
          httpGet:
            path: /opennms/login.jsp
            port: http
          initialDelaySeconds: 30
          periodSeconds: 60
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - 'echo "Shutting down opennms..." && echo 1 > /opt/opennms/logs/opennms.pid && /opt/opennms/bin/opennms stop'
      # Optional image to share configuration directory through NFS (required by external UI servers)
      - name: nfs
        image: itsthenetwork/nfs-server-alpine:11
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        env:
        - name: SHARED_DIRECTORY
          value: /opennms-etc
        ports:
        - containerPort: 2049
          name: nfs
        volumeMounts:
        - name: opennms-etc
          mountPath: /opennms-etc
      volumes:
      - name: opennms-config
        configMap:
          name: opennms-config
  volumeClaimTemplates:
  - metadata:
      name: opennms-etc
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: ssd
      resources:
        requests:
          storage: 1Gi # Cannnot be less than that in AWS for an EBS Volume
